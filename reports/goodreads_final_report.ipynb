{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Goodreads recommendation system\n",
    "**Capstone Project 2 Final Report**\n",
    "\n",
    "## Background\n",
    "\n",
    "Goodreads is a social media platform that allows users to rate and review books as well as see what their friends are reading, rating, and reviewing.\n",
    "\n",
    "Users can use the platform to keep track of the books that they have read while also identifying books they'd like to read next. In addition to pulling reading recommendations from their friends' profiles, they can also receive book recommendations through the Goodreads recommender system. However, based on personal experience, the system does not always offer the most helpful suggestions. Most of the books it recommends are obscure and do not appear to be based on what a user previously rated or what their friends have read. \n",
    "\n",
    "Seeing as Amazon purchased Goodreads in 2013, this seems like a huge missed opportunity to provide useful book recommendations that could turn into book sales.  \n",
    "\n",
    "Is it possible to create a more useful recommendation system for readers? \n",
    "\n",
    "## The Data \n",
    "\n",
    "To answer this question I will be using the Goodbooks __[datasets](https://github.com/zygmuntz/goodbooks-10k)__ provided by Github user __[zygmuntz](https://github.com/zygmuntz)__. \n",
    "\n",
    "The Goodbooks dataset includes over six million ratings of ten thousand books on Goodreads. It is separated into three different files:\n",
    "\n",
    "* **Ratings:** Contains nearly 6 million user ratings from 53,424 users \n",
    "* **To-Read:** Contains nearly 1 million books that users added to their 'to-read' shelf \n",
    "* **Books:** Contains all of the meta data for 10,000 books. The metadata includes: title, author, number of ratings, number of each type of rating, and more \n",
    "\n",
    "For the purposes of this project, I will be using the Ratings and Books datasets.\n",
    "\n",
    "### Loading the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# ----------------\n",
    "\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Numpy\n",
    "import numpy as np\n",
    "\n",
    "# SKLearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# Matplotlib\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Datasets\n",
    "# ----------------\n",
    "\n",
    "# Ratings\n",
    "ratings_data = \"../data/raw/ratings.csv\"\n",
    "ratings = pd.read_csv(ratings_data)\n",
    "\n",
    "# Books\n",
    "books_data = \"../data/raw/books.csv\"\n",
    "books = pd.read_csv(books_data)\n",
    "books = books[['book_id', 'title']] ## I only need the book_id and title columns for modeling \n",
    "\n",
    "# Merge ratings and books\n",
    "merged = pd.merge(books, ratings)\n",
    "\n",
    "print(\"Number of books in dataset: \",len(merged.book_id.value_counts()))\n",
    "print(\"Number of users in dataset: \",len(merged.user_id.value_counts()))\n",
    "merged.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Before I start recommending books, let's explore the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the number of reviews per user\n",
    "eda = merged\n",
    "eda['number_of_ratings_user'] = eda['user_id'].groupby(eda['user_id']).transform('count')\n",
    "\n",
    "plt.hist(eda.number_of_ratings_user, alpha = 0.6, color = 'g')\n",
    "plt.xlabel('number of ratings')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('Number of Ratings per User')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/ameenamarie/Goodreads-Recommendation-System/blob/master/reports/figures/fig3.png?raw=true}\" title=\"Weighted Score\" width=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of ratings from users\n",
    "plt.hist(eda.rating, alpha = 0.6, color='g')\n",
    "plt.xlabel('rating')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('Distribution of Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/ameenamarie/Goodreads-Recommendation-System/blob/master/reports/figures/fig4.png?raw=true}\" title=\"Weighted Score\" width=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the ratings count for the 10 most popular books\n",
    "eda['number_of_ratings_book'] = eda['book_id'].groupby(eda['book_id']).transform('count')\n",
    "popular = eda.sort_values('number_of_ratings_book', ascending=False)\n",
    "\n",
    "# Drop duplicate books\n",
    "popular = popular.drop_duplicates(subset='book_id', keep=\"first\")\n",
    "popular = popular[['book_id', 'title', 'number_of_ratings_book']]\n",
    "popular.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a simple model\n",
    "\n",
    "A simple model recommends the same books to all users regardless of their reading or rating history. The recommendations are usually based on the most popular or highest rated items or a score that combines the two. For my simple model, I will be giving each book a weighted score. This will prevent a book with only 10 reviews that are all 5 stars from skewing my results.\n",
    "\n",
    "To build my model I will be adapting DataCamp's _[IMDB 250 tutorial](https://www.datacamp.com/community/tutorials/recommender-systems-python)_ which gives each item a weighted score using the following formula: \n",
    "\n",
    "<img src=\"https://github.com/ameenamarie/Goodreads-Recommendation-System/blob/master/reports/figures/image1.png?raw=true}\" title=\"Weighted Score\" width=400 />\n",
    "\n",
    "* _v_ number of ratings a book has received;\n",
    "* _m_ the minimum number of ratings a book needs to receive to be included in the model;\n",
    "* _R_ the average rating for the book; And\n",
    "* _C_ the average rating for all books\n",
    "\n",
    "To start I created a column with the average number of ratings that a book received (*v*) and the average rating that each book received (*R*). I then calculated the average rating for all books (*C*) and identified the books that fall in the 90th quantile (*m*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a column for the number of ratings a book has received called 'ratings_count'\n",
    "merged['ratings_count'] = merged['book_id'].groupby(merged['book_id']).transform('count')\n",
    "\n",
    "# Create a column for the average rating a book has received called 'average_rating'\n",
    "merged['average_rating'] = merged['rating'].groupby(merged['book_id']).transform('mean')\n",
    "\n",
    "# Calculate the average rating for all books\n",
    "C = merged['rating'].mean()\n",
    "\n",
    "# Calculate the minimum number of ratings a book needs to receive in order to be included in the model\n",
    "m = merged['ratings_count'].quantile(0.90)\n",
    "\n",
    "# Filter the dataset based on value m\n",
    "filtered = merged.copy().loc[merged['ratings_count'] >= m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then built a function that gives a weighted score to each book using the formula above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that gives a weighted score to each book\n",
    "def weighted_score(x, m=m, C=C):\n",
    "    v = x['ratings_count']\n",
    "    R = x['average_rating']\n",
    "    # Calculation based on an IMDB formula \n",
    "    return (v/(v+m) * R) + (m/(m+v) * C)\n",
    "\n",
    "# Create a 'score' column and give each book a weighted score\n",
    "filtered['score'] = filtered.apply(weighted_score, axis=1)\n",
    "filtered.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once every book had a score, I dropped the duplicates and ranked the books by score. I then identified the ten books with the higest score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort books based on score\n",
    "top10 = filtered.sort_values('score', ascending=False)\n",
    "\n",
    "# Drop duplicate books\n",
    "top10 = top10.drop_duplicates(subset='book_id', keep=\"first\")\n",
    "\n",
    "#Print the top 10 books\n",
    "top10[['title', 'ratings_count', 'average_rating', 'score']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! It looks like everyone really loves Harry Potter. \n",
    "\n",
    "If I were to use this model as my recommendation system, Goodreads would recommend these 10 books to every user regardless of their rating or reading history. However, I do not want to recommend Harry Potter to every single user, so I'll be building a collaborative model to get more specific recommendations. \n",
    "\n",
    "I can use this simple recommendation model to recommend books to \"cold start\" users. That is, users who have just joined Goodreads and do not yet have a reading or rating history on the platform. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a collaborative model from scratch\n",
    "\n",
    "Collaborative models make recommendations by comparing users and books to each other and grouping by similarity. This is accomplished by measuring the distance between each user and each item in order to group similar users and items together. The model makes multiple calculations for every item in the dataset, so it is best to have a smaller dataset. \n",
    "\n",
    "I started by filtering the data to only include users that have reviewed at least 175 books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter data to only include users who have reviewed at least 175 books\n",
    "collab = merged\n",
    "collab['number_of_reviews'] = collab['user_id'].groupby(collab['user_id']).transform('count') \n",
    "collab = collab[collab['number_of_reviews'] >= 175]\n",
    "collab = collab.drop('number_of_reviews', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I removed users who have rated fewer than 175 books, I had to create a new user index and a new book index so that I could place the ratings for each user and book into a matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for user_index and book_index\n",
    "collab = collab.assign(user_index=(collab['user_id']).astype('category').cat.codes)\n",
    "collab = collab.assign(book_index=(collab['book_id']).astype('category').cat.codes)\n",
    "\n",
    "n_users = collab.user_id.unique().shape[0]\n",
    "n_items = collab.book_id.unique().shape[0]\n",
    "print('DataFrame shape: {}'.format(collab.shape))\n",
    "print('Number of unique users:', n_users)\n",
    "print('Number of books:', n_items)\n",
    "collab.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collaborative model requires a matrix using the user_index, book_index, and rating in order to measure the distance between each item and user. I created a training and test matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rearrange the order of the columns to prep for modeling\n",
    "collab = collab[['user_index', 'book_index', 'rating', 'book_id', 'title', 'user_id', 'ratings_count',\n",
    "                  'average_rating']]\n",
    "\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "train_data, test_data = train_test_split(collab, test_size=0.25)\n",
    "\n",
    "#Create two user-item matrices, one for training and another for testing\n",
    "train_data_matrix = np.zeros((n_users, n_items))\n",
    "for line in train_data.itertuples():\n",
    "    train_data_matrix[line[1]-1, line[2]-1] = line[3]  \n",
    "\n",
    "test_data_matrix = np.zeros((n_users, n_items))\n",
    "for line in test_data.itertuples():\n",
    "    test_data_matrix[line[1]-1, line[2]-1] = line[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once everything was placed in a train and test matrix, I was able to calculate the similarity between each user or item using pairwise_distance. I used the cosine metric which looks at the cosine of the angle between each item or user. The closer two items are, the larger the cosine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the similarity using pairwise_distance\n",
    "user_similarity = pairwise_distances(train_data_matrix, metric='cosine')\n",
    "item_similarity = pairwise_distances(train_data_matrix.T, metric='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm ready to make predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to make predictions\n",
    "def predict(ratings, similarity, type='user'):\n",
    "    if type == 'user':\n",
    "        mean_user_rating = ratings.mean(axis=1)\n",
    "        #We use np.newaxis so that mean_user_rating has same format as ratings\n",
    "        ratings_diff = (ratings - mean_user_rating[:, np.newaxis])\n",
    "        pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    elif type == 'item':\n",
    "        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make Predictions\n",
    "user_prediction = predict(train_data_matrix, user_similarity, type='user')\n",
    "item_prediction = predict(train_data_matrix, item_similarity, type='item')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model \n",
    "\n",
    "Let's see how the model performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "def rmse(prediction, ground_truth):\n",
    "    prediction = prediction[ground_truth.nonzero()].flatten() \n",
    "    ground_truth = ground_truth[ground_truth.nonzero()].flatten()\n",
    "    return sqrt(mean_squared_error(prediction, ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('User-based CF RMSE: ' + str(rmse(user_prediction, test_data_matrix)))\n",
    "print('Item-based CF RMSE: ' + str(rmse(item_prediction, test_data_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Now let's see what kind of recommendations we can get. I'll be printing some recommendations for user 487."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a function for making predictions\n",
    "def collab_rec(predictions_df, user_id, books_df, original_ratings_df, num_recommendations=5):\n",
    "    \n",
    "    # Get and sort the user's predictions\n",
    "    user_row_number = user_id - 1 # UserID starts at 1, not 0\n",
    "    sorted_user_predictions = predictions_df.iloc[user_row_number].sort_values(ascending=False)\n",
    "    \n",
    "    preds = pd.DataFrame(sorted_user_predictions)\n",
    "    preds['book_id'] = preds.index + 1\n",
    "    \n",
    "    # Get the user's data and merge in the book information.\n",
    "    user_data = original_ratings_df[original_ratings_df.user_id == (user_id)]\n",
    "    user_full = (user_data.merge(books_df, how = 'left', left_on = 'book_id', right_on = 'book_id').\n",
    "                     sort_values(['rating'], ascending=False)\n",
    "                 )\n",
    "\n",
    "    print('User {0} has already rated {1} books.'.format(user_id, user_full.shape[0]))\n",
    "    print('Recommending the highest {0} predicted ratings books not already rated.'.format(num_recommendations))\n",
    "    \n",
    "    # Recommend the highest predicted rating books that the user hasn't seen yet.\n",
    "    recommendations = (books_df[~books_df['book_id'].isin(user_full['book_id'])].\n",
    "         merge(pd.DataFrame(preds).reset_index(), how = 'left',\n",
    "               left_on = 'book_id',\n",
    "              right_on = 'book_id').\n",
    "         rename(columns = {user_row_number: 'Predictions'}).\n",
    "         sort_values('Predictions', ascending = False).\n",
    "                       iloc[:num_recommendations, :-1]\n",
    "                      )\n",
    "\n",
    "    return user_full, recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions \n",
    "user_df = pd.DataFrame(user_prediction)\n",
    "\n",
    "user_read, recommendations = collab_rec(user_df, 487, books, ratings, 10)\n",
    "\n",
    "# print books already read by user 487\n",
    "user_read[['title']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print recommendations for user 487\n",
    "recommendations[['title']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on what this user has previously read, I would say these are decent recommendations. Let's see if we can do better with Singular Vector Decomposition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Vector Decomposition Model\n",
    "\n",
    "The Singular Vector Decomposition model breaks a matrix (like the one used to create the collaborative recommendations above) into its component parts. This allows for more mathematically complex calculations on the matrix later in the modeling process. The matrix is broken down as follows: \n",
    "\n",
    "<img src=\"https://github.com/ameenamarie/Goodreads-Recommendation-System/blob/master/reports/figures/image2.png?raw=true}\" title=\"SVD Formula\", width=150 />\n",
    "\n",
    "* *R* is the user ratings matrix;\n",
    "* *U* is the user \"features\" matrix;\n",
    "* *Î£* is the diagonal matrix of singular values (weights); And\n",
    "* *Vt* is the books features matrix \n",
    "\n",
    "Before I can break the matrix down into its component parts, I need to filter the dataset to only include users who have rated at least 175 books like I did with the collaborative model. \n",
    "\n",
    "I will then convert the dataframe into a pivot table with one row for every user and one column for every book. The value in the pivot table is the rating that the user gave a particular book. This pivot table will eventually become the matrix that I will break down. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = merged\n",
    "\n",
    "# Filter data to only include users who have reviewed at least 175 books \n",
    "svd['number_of_reviews'] = svd['user_id'].groupby(svd['user_id']).transform('count') \n",
    "svd = svd[svd['number_of_reviews'] >= 175]\n",
    "svd = svd.drop('number_of_reviews', 1)\n",
    "\n",
    "# Create a pivot table of users and ratings\n",
    "pivot = svd.pivot(index = 'user_id', columns ='book_id', values = 'rating').fillna(0)\n",
    "pivot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the matrix, I calculated each user's average rating and then replaced their rating in the matrix with the result of subtracting their average rating from the rating that they gave each book. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pivot table to a matrix\n",
    "R = pivot.as_matrix()\n",
    "\n",
    "# Identify a user's average rating \n",
    "user_ratings_mean = np.mean(R, axis = 1)\n",
    "R_demeaned = R - user_ratings_mean.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am now ready to pass the values through the svds model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "U, sigma, Vt = svds(R_demeaned, k = 50)\n",
    "\n",
    "sigma = np.diag(sigma)\n",
    "\n",
    "all_user_predicted_ratings = np.dot(np.dot(U, sigma), Vt) + user_ratings_mean.reshape(-1, 1)\n",
    "preds_df = pd.DataFrame(all_user_predicted_ratings, columns = pivot.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that I have the predicted ratings for each book, I can start making recommendations. I started by building a function that identifies the row for the desired user, sorts their predicted ratings, removes the books they've already read, and then from the remaining predicted ratings returns the top values as recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_books(predictions_df, user_id, books_df, original_ratings_df, num_recommendations=5):\n",
    "    \n",
    "    # Get and sort the user's predictions\n",
    "    user_row_number = user_id - 1 # UserID starts at 1, not 0\n",
    "    sorted_user_predictions = predictions_df.iloc[user_row_number].sort_values(ascending=False)\n",
    "    \n",
    "    # Get the user's data and merge in the book information.\n",
    "    user_data = original_ratings_df[original_ratings_df.user_id == (user_id)]\n",
    "    user_full = (user_data.merge(books_df, how = 'left', left_on = 'book_id', right_on = 'book_id').\n",
    "                     sort_values(['rating'], ascending=False)\n",
    "                 )\n",
    "\n",
    "    print('User {0} has already rated {1} books.'.format(user_id, user_full.shape[0]))\n",
    "    print('Recommending the highest {0} predicted ratings books not already rated.'.format(num_recommendations))\n",
    "    \n",
    "    # Recommend the highest predicted rating books that the user hasn't seen yet.\n",
    "    recommendations = (books_df[~books_df['book_id'].isin(user_full['book_id'])].\n",
    "         merge(pd.DataFrame(sorted_user_predictions).reset_index(), how = 'left',\n",
    "               left_on = 'book_id',\n",
    "              right_on = 'book_id').\n",
    "         rename(columns = {user_row_number: 'Predictions'}).\n",
    "         sort_values('Predictions', ascending = False).\n",
    "                       iloc[:num_recommendations, :-1]\n",
    "                      )\n",
    "\n",
    "    return user_full, recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to once again test the function on User 487:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_rated, recommendations = recommend_books(preds_df, 487, books, ratings, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View books the user has already rated\n",
    "already_rated[['title']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the 10 recommended books for User 487\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The three models used above only used ratings to determine recommendations. I would like to try and build a model that incorporates book features such as the year of publication, author, genre, and title in order to make more specific recommendations for each user.\n",
    "\n",
    "I can do this using natural language processing to look for similar words and phrases to group books and make recommendations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
