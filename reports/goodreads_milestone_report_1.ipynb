{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Goodreads recommendation system\n",
    "**Capstone Project 2 Milestone Report 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Goodreads is a social media platform that allows users (aka readers) to rate and review books as well as see what their friends are reading, rating, and reviewing. Readers can store books on ‘shelves’ based on what they’ve read or what they want to read. \n",
    "\n",
    "Currently, the Goodreads recommendation system uses a reader’s shelves to suggest books they might be interested in reading next. However, the system is rudimentary and does not offer helpful suggestions since most of the books it recommends are obscure and do not appear to be based on what they've previously rated or what their friends have read.\n",
    "\n",
    "Is it possible to create a more useful recommendation system for readers? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data \n",
    "\n",
    "To answer this question I will be using the Goodbooks __[datasets](https://github.com/zygmuntz/goodbooks-10k)__ provided by Github user __[zygmuntz](https://github.com/zygmuntz)__. \n",
    "\n",
    "The Goodbooks dataset includes over six million ratings of ten thousand books on Goodreads. It is separated into three different files \n",
    "\n",
    "* **Ratings:** Contains nearly 6 million user ratings from 53,424 users \n",
    "* **To-Read:** Contains nearly 1 million books that users added to their 'to-read' shelf \n",
    "* **Books:** Contains all of the meta data for 10,000 books. The metadata includes: title, author, number of ratings, number of each type of rating, and more \n",
    "\n",
    "### Data Cleaning\n",
    "\n",
    "In order to prep the data for the model I performed a few data cleaning techniques. The ratings and to_read datasets were both clean with no missing data, so all steps were performed on the books dataset. \n",
    "\n",
    "**Data Cleaning Steps**\n",
    "1. Dropped the isbn and image url columns since they will not be used in the model\n",
    "2. 21 books were missing the publication year. Since it does not matter which year I replace the missing information with, I decided to give these books a publication year of 1984. \n",
    "3. The dataset has a column for original_title (i.e. The Hunger Games) and title (i.e. The Hunger Games (The Hunger Games #1)). 585 books were missing original_title information, so I replaced those NaNs with the title. \n",
    "4. Nearly 2000 books were missing language codes, so I replaced those with 'Unknown'\n",
    "5. I added a column to the ratings dataset for the average rating per user \n",
    "6. I added a column to the ratings dataset for the number of ratings per user \n",
    "\n",
    "Once the datasets were cleaned and prepped, I merged all three into a single dataframe. I added a column called 'read_unread' that marked whether a row came from the ratings dataset or from the to_read dataset. \n",
    "\n",
    "The merged dataframe had over 6 million rows. I do not need this many rows in order to create my model so I filtered the dataframe down include only users that have rated over 145 books and books that have at least 100,000 reviews. This resulted in a dataframe with 504,756 rows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of average rating per book and average rating per user is a normal distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"figures/fig1.png\" height=500 width =500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig2.png\" height=500 width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that the data has been merged, cleaned, and explored my next steps are to use item-item collaborative filtering, user-user collaborative filtering, and Single Value Decomposition to see which one creates the simplest and strongest model for prediction. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
